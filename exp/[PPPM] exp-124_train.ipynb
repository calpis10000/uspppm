{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655353117426,"user":{"displayName":"堂込一智","userId":"12219727497971505625"},"user_tz":-540},"id":"ODefLcHTuzkZ","outputId":"7d683fd0-ba32-435e-802b-9b0425bbf1d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jun 16 04:18:36 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11765,"status":"ok","timestamp":1655353219230,"user":{"displayName":"堂込一智","userId":"12219727497971505625"},"user_tz":-540},"id":"bfwZudNPMeuX","outputId":"8f7b3622-d159-4fe0-e037-2ee2bb84572c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.19.1\n","  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 13.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.1) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.1) (4.64.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 67.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.1) (3.7.1)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 55.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.1) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.1) (4.11.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.1) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.1) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.1) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.1) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.1) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.1) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.1) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece==0.1.96\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 16.5 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["!pip install transformers==4.19.1\n","!pip install sentencepiece==0.1.96"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-O7K0BPfBHO"},"outputs":[],"source":["# ----------------------------------------------\n","# Load Libraries\n","# ----------------------------------------------\n","import pathlib\n","from pathlib import Path\n","import sys\n","import os\n","import math\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.nn import functional as F\n","\n","from transformers import AdamW\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import BertForSequenceClassification, BertConfig, BertModel\n","from transformers import get_cosine_schedule_with_warmup\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","\n","from tqdm import tqdm\n","\n","import gc\n","gc.enable()"]},{"cell_type":"markdown","metadata":{"id":"lLCSxR_tfbtn"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nv0CdG49fTXR"},"outputs":[],"source":["class CFG:\n","    exp_id = 'exp124'\n","    input_path = 'input/'\n","    cpc_path = 'input/'\n","    model_path = 'microsoft/deberta-v3-large'\n","    out_base = 'output'\n","    out_path = f'{out_base}/{exp_id}'\n","    model_save_path = f'{out_path}/models'\n","    \n","    debug = False\n","    fold1_only = False\n","    upload_dataset = True\n","    debug_size = 100\n","    log_interval = 1822 # 未使用\n","    seed = 8\n","    max_len = 92\n","    learning_rate = 2e-5\n","    weight_decay = 0.01\n","    lr_decay = 0.98\n","    num_classes = 5\n","    num_fold = 4\n","    epochs = 5\n","    train_batch_size = 16\n","    valid_batch_size = 16\n","    gradient_accumulation_step = 1\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    if debug:\n","      epochs = 1\n","      upload_dataset = False\n","\n","    if fold1_only:\n","      upload_dataset = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2LXX-6iO5Re"},"outputs":[],"source":["!mkdir -p {CFG.out_path}\n","!mkdir -p {CFG.model_save_path}"]},{"cell_type":"markdown","metadata":{"id":"1M170jR7fcj8"},"source":["# Preproc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iK90exIjNouV"},"outputs":[],"source":["# ----------------------------------------------\n","# Set SEED\n","# ----------------------------------------------\n","# seed\n","SEED = CFG.seed\n","def set_seed(SEED):\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    os.environ['PYTHONHASHSEED'] = str(SEED)\n","    \n","    torch.manual_seed(SEED)\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.deterministic = True\n","    \n","set_seed(SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":363,"status":"ok","timestamp":1655329454281,"user":{"displayName":"堂込一智","userId":"12219727497971505625"},"user_tz":-540},"id":"yOJIHPpJikW3","outputId":"1c1cf13f-162f-4e99-d692-7a0857f10801"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id     anchor                  target context  score  \\\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50   \n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75   \n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25   \n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50   \n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00   \n","\n","                                        context_text  \n","0  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...  \n","1  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...  \n","2  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...  \n","3  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...  \n","4  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...  "],"text/html":["\n","  <div id=\"df-4a587cfa-e927-48d7-8116-153243910a07\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","      <th>context_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a587cfa-e927-48d7-8116-153243910a07')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4a587cfa-e927-48d7-8116-153243910a07 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4a587cfa-e927-48d7-8116-153243910a07');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}],"source":["train_df = pd.read_csv(f\"{CFG.input_path}train.csv\")\n","# https://www.kaggle.com/code/gauravbrills/folds-dump-the-two-paths-fix\n","cpc_texts = torch.load(CFG.cpc_path+\"cpc_texts_fixed.pth\")\n","train_df['context_text'] = train_df['context'].map(cpc_texts)\n","train_df['context_text'] = \"[context=\" + train_df['context'] + ']' + train_df['context_text']\n","train_df['context_text'] = \"[subgrp=\" + train_df['context'].map(lambda x: x[:1]) + ']' + train_df['context_text']\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2668,"status":"ok","timestamp":1655329456945,"user":{"displayName":"堂込一智","userId":"12219727497971505625"},"user_tz":-540},"id":"OESlEphOfUcZ","outputId":"3dfff00b-5b3c-49c7-f9fb-5b249325b2a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["550 183\n","549 184\n","550 183\n","550 183\n"]}],"source":["!pip install -q iterative-stratification\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","dfx = pd.get_dummies(train_df, columns=[\"score\"]).groupby([\"anchor\"], as_index=False).sum()\n","cols = [c for c in dfx.columns if c.startswith(\"score_\") or c == \"anchor\"]\n","dfx = dfx[cols]\n","\n","mskf = MultilabelStratifiedKFold(n_splits=CFG.num_fold, shuffle=True, random_state=42)\n","labels = [c for c in dfx.columns if c != \"anchor\"]\n","dfx_labels = dfx[labels]\n","dfx[\"fold\"] = -1\n","\n","for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\n","    print(len(trn_), len(val_))\n","    dfx.loc[val_, \"fold\"] = fold\n","\n","train_df = train_df.merge(dfx[[\"anchor\", \"fold\"]], on=\"anchor\", how=\"left\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1655329456946,"user":{"displayName":"堂込一智","userId":"12219727497971505625"},"user_tz":-540},"id":"sGikoiA9HfG2","outputId":"400fd62b-d4d2-4902-f714-2964333e7494"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    9379\n","1    8860\n","2    8612\n","3    9622\n","Name: fold, dtype: int64"]},"metadata":{},"execution_count":14}],"source":["train_df['fold'].value_counts().sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655329456946,"user":{"displayName":"堂込一智","userId":"12219727497971505625"},"user_tz":-540},"id":"qskUdzrFG5uq","outputId":"cd1610e5-a9e1-4ca2-a736-6dd18ffc5e69"},"outputs":[{"output_type":"stream","name":"stdout","text":["(36473, 7)\n"]},{"output_type":"execute_result","data":{"text/plain":["                 id     anchor                  target context  score  \\\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50   \n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75   \n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25   \n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50   \n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00   \n","\n","                                        context_text  fold  \n","0  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0  \n","1  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0  \n","2  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0  \n","3  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0  \n","4  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0  "],"text/html":["\n","  <div id=\"df-32499c09-562b-465e-819b-a63c97b27f78\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","      <th>context_text</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32499c09-562b-465e-819b-a63c97b27f78')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-32499c09-562b-465e-819b-a63c97b27f78 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-32499c09-562b-465e-819b-a63c97b27f78');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}],"source":["print(train_df.shape)\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9BTOQ441fgW6"},"outputs":[],"source":["train_df['input'] = train_df['anchor'] + '[SEP]' + train_df['target'] + '[SEP]' + train_df['context_text']"]},{"cell_type":"code","source":["train_df = pd.concat([train_df, pd.get_dummies(train_df['score'])], axis='columns')"],"metadata":{"id":"xClRI0Fa9_p9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df[[0.0,0.25,0.5,0.75,1.0]].values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gM-fBBL1-iDw","executionInfo":{"status":"ok","timestamp":1655329457549,"user_tz":-540,"elapsed":11,"user":{"displayName":"堂込一智","userId":"12219727497971505625"}},"outputId":"e843e9a5-8826-4a65-fe85-43082ee75247"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 1, 0, 0],\n","       [0, 0, 0, 1, 0],\n","       [0, 1, 0, 0, 0],\n","       ...,\n","       [0, 0, 1, 0, 0],\n","       [0, 0, 0, 1, 0],\n","       [0, 0, 1, 0, 0]], dtype=uint8)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655329457549,"user":{"displayName":"堂込一智","userId":"12219727497971505625"},"user_tz":-540},"id":"tjNzFNT8noYm","outputId":"eb191a5e-b863-42cc-dff5-fe13dbcf9b46"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 id     anchor                  target context  score  \\\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50   \n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75   \n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25   \n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50   \n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00   \n","\n","                                        context_text  fold  \\\n","0  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0   \n","1  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0   \n","2  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0   \n","3  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0   \n","4  [subgrp=A][context=A47]HUMAN NECESSITIES. FURN...     0   \n","\n","                                               input  0.0  0.25  0.5  0.75  \\\n","0  abatement[SEP]abatement of pollution[SEP][subg...    0     0    1     0   \n","1  abatement[SEP]act of abating[SEP][subgrp=A][co...    0     0    0     1   \n","2  abatement[SEP]active catalyst[SEP][subgrp=A][c...    0     1    0     0   \n","3  abatement[SEP]eliminating process[SEP][subgrp=...    0     0    1     0   \n","4  abatement[SEP]forest region[SEP][subgrp=A][con...    1     0    0     0   \n","\n","   1.0  \n","0    0  \n","1    0  \n","2    0  \n","3    0  \n","4    0  "],"text/html":["\n","  <div id=\"df-a4386a01-46e2-44e5-be96-4940383c574b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","      <th>context_text</th>\n","      <th>fold</th>\n","      <th>input</th>\n","      <th>0.0</th>\n","      <th>0.25</th>\n","      <th>0.5</th>\n","      <th>0.75</th>\n","      <th>1.0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","      <td>abatement[SEP]abatement of pollution[SEP][subg...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","      <td>abatement[SEP]act of abating[SEP][subgrp=A][co...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","      <td>abatement[SEP]active catalyst[SEP][subgrp=A][c...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","      <td>abatement[SEP]eliminating process[SEP][subgrp=...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","      <td>[subgrp=A][context=A47]HUMAN NECESSITIES. FURN...</td>\n","      <td>0</td>\n","      <td>abatement[SEP]forest region[SEP][subgrp=A][con...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4386a01-46e2-44e5-be96-4940383c574b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a4386a01-46e2-44e5-be96-4940383c574b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a4386a01-46e2-44e5-be96-4940383c574b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}],"source":["train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"aAVnHp8Ofsso"},"source":["# Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1775,"status":"ok","timestamp":1655329459319,"user":{"displayName":"堂込一智","userId":"12219727497971505625"},"user_tz":-540},"id":"r1ADD0EJfgUj","outputId":"368667cc-e4e3-4fbe-fb55-b0e7f38d3a11"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/usr/local/lib/python3.7/dist-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"execute_result","data":{"text/plain":["128145"]},"metadata":{},"execution_count":20}],"source":["subgrp = train_df['context'].map(lambda x: x[:1]).unique()\n","\n","tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n","tokenizer.add_tokens([f\"[context={i}]\" for i in cpc_texts.keys()])\n","tokenizer.add_tokens([f\"[subgrp={i}]\" for i in subgrp])\n","len(tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1655329459320,"user":{"displayName":"堂込一智","userId":"12219727497971505625"},"user_tz":-540},"id":"h30ob5ihINOq","outputId":"e039fc10-e443-4a63-ec27-af462fa2095a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ntoken_len = train_df['input'].map(lambda x: tokenizer(x)['input_ids'].__len__())\\ndisplay(token_len.describe())\\ntoken_len.hist(bins=100)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["# トークン長分布の確認\n","\"\"\"\n","token_len = train_df['input'].map(lambda x: tokenizer(x)['input_ids'].__len__())\n","display(token_len.describe())\n","token_len.hist(bins=100)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"n5qNQG7XfvtE"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r5JyZ3RnfgRr"},"outputs":[],"source":["def prepare_input(tokenizer, text):\n","    inputs = tokenizer(text,\n","                           add_special_tokens=True,\n","                           max_length=CFG.max_len,\n","                           padding=\"max_length\",\n","                           truncation=True,\n","                           return_offsets_mapping=False)\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, df):\n","        self.inputs = df['input'].values\n","        #self.label = df['score'].values\n","        self.label = df[[0.0,0.25,0.5,0.75,1.0]].values\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, item):\n","        inputs = self.inputs[item]\n","        label = self.label[item]\n","        outputs = prepare_input(tokenizer, inputs)\n","        outputs['label'] = torch.tensor(label, dtype=torch.float32)\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"ikO_UwLyf1KF"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTGp5AHE2e4o"},"outputs":[],"source":["# ----------------------------------------------\n","# Model\n","# ----------------------------------------------\n","class AttentionHead(nn.Module):\n","    def __init__(self, in_features, hidden_dim, num_targets):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.middle_features = hidden_dim\n","        self.W = nn.Linear(in_features, hidden_dim)\n","        self.V = nn.Linear(hidden_dim, 1)\n","        self.out_features = hidden_dim\n","\n","    def forward(self, features):\n","        att = torch.tanh(self.W(features))\n","        score = self.V(att)\n","        attention_weights = torch.softmax(score, dim=1)\n","        context_vector = attention_weights * features\n","        context_vector = torch.sum(context_vector, dim=1)\n","\n","        return context_vector\n","\n","class PPPMModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.config = AutoConfig.from_pretrained(CFG.model_path)\n","        self.pre_model = AutoModel.from_pretrained(CFG.model_path, config=self.config)\n","        self.pre_model.resize_token_embeddings(len(tokenizer))\n","        self.head = AttentionHead(self.config.hidden_size, self.config.hidden_size,1)\n","        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n","        self.dropout1 = nn.Dropout(0.1)\n","        self.dropout2 = nn.Dropout(0.2)\n","        self.dropout3 = nn.Dropout(0.3)\n","        self.dropout4 = nn.Dropout(0.4)\n","        self.dropout5 = nn.Dropout(0.5)\n","        self.regressor = nn.Linear(self.config.hidden_size, CFG.num_classes)\n","    \n","    def forward(self, inputs):\n","        pre_out = self.pre_model(**inputs)\n","        last_hidden_states = pre_out[0]\n","        last_hidden_states = self.dropout(self.head(last_hidden_states))\n","        logits1 = self.regressor(self.dropout1(last_hidden_states))\n","        logits2 = self.regressor(self.dropout2(last_hidden_states))\n","        logits3 = self.regressor(self.dropout3(last_hidden_states))\n","        logits4 = self.regressor(self.dropout4(last_hidden_states))\n","        logits5 = self.regressor(self.dropout5(last_hidden_states))\n","        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfqjFdzk3yd3"},"outputs":[],"source":["# ----------------------------------------------\n","# func: valid, predict\n","# ----------------------------------------------\n","def predict(model, dataloader):\n","    model.eval()\n","    result = np.zeros((len(dataloader.dataset), CFG.num_classes))\n","    idx = 0\n","    \n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(dataloader):\n","          inputs = {}\n","          for k, v in data.items():\n","            if k != 'label':\n","              inputs[k] = v.to(CFG.device)            \n","          output = model(inputs)\n","          output = nn.Softmax(dim=1)(output)\n","          result[idx:idx + output.shape[0], :] = output.to('cpu')\n","          idx += output.shape[0]\n","            \n","    return result\n","\n","\n","def valid_func(model, dataloader):\n","    model.eval()\n","    result = np.zeros((len(dataloader.dataset), CFG.num_classes))\n","    idx = 0\n","    loss_sum = 0\n","    bar = tqdm(dataloader, total=len(dataloader))\n","    \n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(bar):\n","          inputs = {}\n","          for k, v in data.items():\n","            if k != 'label':\n","              inputs[k] = v.to(CFG.device)            \n","          label = data['label'].to(CFG.device)          \n","          output = model(inputs)\n","          output = nn.Softmax(dim=1)(output)\n","          result[idx:idx + output.shape[0], :] = output.to('cpu')\n","          idx += output.shape[0]\n","\n","          loss_sum += nn.CrossEntropyLoss(reduction='sum')(output, label).item()\n","            \n","    return loss_sum/(len(dataloader.dataset)), result.reshape((len(dataloader.dataset), CFG.num_classes))\n","\n","def label_to_score(label):\n","    return (label*[0,0.25,0.5,0.75,1.0]).sum(axis=1)\n","\n","def metric_pearson(predictions, labels):\n","    pred_score = label_to_score(predictions)\n","    label_score = label_to_score(labels)\n","    pearson = np.corrcoef(pred_score, label_score)[0][1]       \n","    return pearson"]},{"cell_type":"code","source":["\"\"\"preds = predict(model, valid_loader)\n","label_to_score(valid_loader.dataset.label)\n","metric_pearson(preds, valid_loader.dataset.label)\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"t2JSQz40B_UC","executionInfo":{"status":"ok","timestamp":1655329459322,"user_tz":-540,"elapsed":11,"user":{"displayName":"堂込一智","userId":"12219727497971505625"}},"outputId":"0debaf70-fca6-4bae-eb35-1a596c4b71a1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'preds = predict(model, valid_loader)\\nlabel_to_score(valid_loader.dataset.label)\\nmetric_pearson(preds, valid_loader.dataset.label)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0HFdCII38i_"},"outputs":[],"source":["# ----------------------------------------------\n","# func: train\n","# ----------------------------------------------\n","def train_fn(\n","    model,\n","    save_path,\n","    train_loader,\n","    val_loader,\n","    optimizer,\n","    scheduler=None,\n","    num_epochs=CFG.epochs\n","):\n","\n","    best_score = 0\n","    best_epoch = 0\n","    running_loss = 0.0\n","    dataset_size = 0\n","    log_interval = CFG.log_interval\n","    oof_preds = None\n","\n","    start = time.time()\n","\n","    for epoch in range(num_epochs):\n","        val_score = None\n","        model.train()\n","        bar = tqdm(train_loader, total=len(train_loader))\n","        for batch_idx, data in enumerate(bar):\n","          inputs = {}\n","          for k, v in data.items():\n","            if k != 'label':\n","              inputs[k] = v.to(CFG.device)            \n","          label = data['label'].to(CFG.device)\n","          batch_size = label.size(0)\n","\n","          output = model(inputs)\n","          loss = nn.CrossEntropyLoss()(output, label)\n","          loss = loss / CFG.gradient_accumulation_step\n","\n","          loss.backward()\n","\n","          if (batch_idx + 1) % CFG.gradient_accumulation_step == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            if scheduler:\n","                scheduler.step()\n","\n","          if CFG.debug == True:\n","            if (batch_idx > 0) & (batch_idx % CFG.debug_size == 0):\n","                break\n","\n","          running_loss += (loss.item() * batch_size)\n","          dataset_size += batch_size            \n","          total_loss = running_loss / dataset_size\n","          bar.set_postfix(Epoch=epoch, Loss=loss.item(), TotalLoss=total_loss, LR=optimizer.param_groups[0]['lr'])\n","\n","        val_start = time.time()\n","        val_score, predictions = valid_func(model, val_loader)\n","        pearson = metric_pearson(predictions, val_loader.dataset.label)\n","        print(f\"Epoch {epoch+1}, Step {batch_idx+1}, train_loss: {loss:0.5f}, val_loss: {val_score:0.5f}, pearson: {pearson:0.5f}\")\n","        if pearson > best_score:\n","            print(f\"Model Inproved: {best_score} ----> {pearson}\")\n","            best_score = pearson\n","            oof_preds = predictions\n","            torch.save(model.state_dict(), save_path)\n","        print(f\"validation elasped time: {time.time() - val_start: 0.3}\")\n","\n","    print(f\"total elasped time: {time.time() - start: 0.3}\")\n","    start = time.time()\n","\n","    return best_score, oof_preds\n","\n","# ----------------------------------------------\n","# create optimizer\n","# ----------------------------------------------\n","def create_optimizer(model):\n","    named_params = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optim_params = []\n","    for idx_, (name_, params_) in enumerate(named_params):\n","        weight_decay = 0 if name_ in no_decay else 0.01\n","        optim_params.append({'params':params_,\n","                            'weight_decay': weight_decay,\n","                            })\n","\n","    return AdamW(optim_params)\n","\n","\n","# https://www.ai-shift.co.jp/techblog/2145\n","def create_optimizer_grouped_parameters(model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters()\n","                       if 'lstm' in n\n","                       or 'cnn' in n\n","                       or 'regressor' in n],\n","            \"weight_decay\": 0.0,\n","            \"lr\": 1e-3,\n","        },\n","    ]\n","    num_layers = model.config.num_hidden_layers\n","    layers = [getattr(model, 'pre_model').embeddings] + list(getattr(model, 'pre_model').encoder.layer)\n","    layers.reverse()\n","    lr = CFG.learning_rate\n","    for layer in layers:\n","        lr *= CFG.lr_decay\n","        optimizer_grouped_parameters += [\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": CFG.weight_decay,\n","                \"lr\": lr,\n","            },\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                \"lr\": lr,\n","            },\n","        ]\n","    return AdamW(optimizer_grouped_parameters)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxz1VyLy6wkn","outputId":"2b0f0577-61b4-42cd-f495-567802b6e179","executionInfo":{"status":"ok","timestamp":1655336347692,"user_tz":-540,"elapsed":6888379,"user":{"displayName":"堂込一智","userId":"12219727497971505625"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["*** FOLD 1 / 4***\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","100%|██████████| 1693/1693 [05:04<00:00,  5.55it/s, Epoch=0, LR=0.000913, Loss=0.903, TotalLoss=0.857]\n","100%|██████████| 587/587 [00:27<00:00, 21.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Step 1693, train_loss: 0.90286, val_loss: 1.25005, pearson: 0.83281\n","Model Inproved: 0 ----> 0.8328123289401977\n","validation elasped time:  32.8\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1693/1693 [05:06<00:00,  5.53it/s, Epoch=1, LR=0.000665, Loss=0.415, TotalLoss=0.712]\n","100%|██████████| 587/587 [00:28<00:00, 20.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Step 1693, train_loss: 0.41523, val_loss: 1.22217, pearson: 0.82621\n","validation elasped time:  28.4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1693/1693 [05:05<00:00,  5.54it/s, Epoch=2, LR=0.000353, Loss=0.25, TotalLoss=0.602]\n","100%|██████████| 587/587 [00:27<00:00, 21.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Step 1693, train_loss: 0.24956, val_loss: 1.19838, pearson: 0.83282\n","Model Inproved: 0.8328123289401977 ----> 0.8328216475590209\n","validation elasped time:  37.8\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1693/1693 [05:03<00:00,  5.57it/s, Epoch=3, LR=9.77e-5, Loss=0.201, TotalLoss=0.512]\n","100%|██████████| 587/587 [00:28<00:00, 20.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Step 1693, train_loss: 0.20072, val_loss: 1.19336, pearson: 0.83001\n","validation elasped time:  28.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1693/1693 [05:05<00:00,  5.54it/s, Epoch=4, LR=0, Loss=0.0647, TotalLoss=0.441]\n","100%|██████████| 587/587 [00:28<00:00, 20.94it/s]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Step 1693, train_loss: 0.06471, val_loss: 1.19216, pearson: 0.82594\n","validation elasped time:  28.0\n","total elasped time:  1.68e+03\n","[0.8328216475590209]\n","Mean: 0.8328216475590209\n","*** FOLD 2 / 4***\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|██████████| 1725/1725 [05:11<00:00,  5.53it/s, Epoch=0, LR=0.000913, Loss=0.449, TotalLoss=0.851]\n","100%|██████████| 554/554 [00:26<00:00, 20.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Step 1725, train_loss: 0.44879, val_loss: 1.25548, pearson: 0.79829\n","Model Inproved: 0 ----> 0.7982900276735344\n","validation elasped time:  53.2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1725/1725 [05:12<00:00,  5.51it/s, Epoch=1, LR=0.000665, Loss=0.44, TotalLoss=0.702]\n","100%|██████████| 554/554 [00:26<00:00, 20.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Step 1725, train_loss: 0.43951, val_loss: 1.23086, pearson: 0.82321\n","Model Inproved: 0.7982900276735344 ----> 0.8232072468655748\n","validation elasped time:  31.5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1725/1725 [05:12<00:00,  5.52it/s, Epoch=2, LR=0.000353, Loss=0.584, TotalLoss=0.588]\n","100%|██████████| 554/554 [00:26<00:00, 20.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Step 1725, train_loss: 0.58444, val_loss: 1.20849, pearson: 0.81789\n","validation elasped time:  27.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1725/1725 [05:12<00:00,  5.52it/s, Epoch=3, LR=9.77e-5, Loss=0.269, TotalLoss=0.496]\n","100%|██████████| 554/554 [00:26<00:00, 20.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Step 1725, train_loss: 0.26851, val_loss: 1.19852, pearson: 0.81368\n","validation elasped time:  26.6\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1725/1725 [05:13<00:00,  5.51it/s, Epoch=4, LR=0, Loss=0.198, TotalLoss=0.426]\n","100%|██████████| 554/554 [00:27<00:00, 20.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Step 1725, train_loss: 0.19784, val_loss: 1.19838, pearson: 0.81062\n","validation elasped time:  27.1\n","total elasped time:  1.73e+03\n","[0.8328216475590209, 0.8232072468655748]\n","Mean: 0.8280144472122979\n","*** FOLD 3 / 4***\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|██████████| 1741/1741 [05:19<00:00,  5.45it/s, Epoch=0, LR=0.000913, Loss=0.525, TotalLoss=0.856]\n","100%|██████████| 539/539 [00:26<00:00, 20.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Step 1741, train_loss: 0.52463, val_loss: 1.22175, pearson: 0.83363\n","Model Inproved: 0 ----> 0.8336283202210487\n","validation elasped time:  51.1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1741/1741 [05:17<00:00,  5.49it/s, Epoch=1, LR=0.000665, Loss=0.483, TotalLoss=0.712]\n","100%|██████████| 539/539 [00:26<00:00, 20.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Step 1741, train_loss: 0.48332, val_loss: 1.21119, pearson: 0.84048\n","Model Inproved: 0.8336283202210487 ----> 0.8404750439073577\n","validation elasped time:  31.1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1741/1741 [05:15<00:00,  5.52it/s, Epoch=2, LR=0.000352, Loss=0.751, TotalLoss=0.602]\n","100%|██████████| 539/539 [00:25<00:00, 20.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Step 1741, train_loss: 0.75134, val_loss: 1.19163, pearson: 0.83959\n","validation elasped time:  25.9\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1741/1741 [05:15<00:00,  5.53it/s, Epoch=3, LR=9.76e-5, Loss=0.299, TotalLoss=0.513]\n","100%|██████████| 539/539 [00:25<00:00, 20.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Step 1741, train_loss: 0.29881, val_loss: 1.18840, pearson: 0.83491\n","validation elasped time:  25.9\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1741/1741 [05:15<00:00,  5.52it/s, Epoch=4, LR=0, Loss=0.332, TotalLoss=0.442]\n","100%|██████████| 539/539 [00:25<00:00, 20.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Step 1741, train_loss: 0.33156, val_loss: 1.18976, pearson: 0.83253\n","validation elasped time:  25.7\n","total elasped time:  1.74e+03\n","[0.8328216475590209, 0.8232072468655748, 0.8404750439073577]\n","Mean: 0.8321679794439846\n","*** FOLD 4 / 4***\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|██████████| 1678/1678 [05:04<00:00,  5.52it/s, Epoch=0, LR=0.000913, Loss=0.485, TotalLoss=0.851]\n","100%|██████████| 602/602 [00:28<00:00, 20.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Step 1678, train_loss: 0.48508, val_loss: 1.24741, pearson: 0.80113\n","Model Inproved: 0 ----> 0.8011305231470817\n","validation elasped time:  55.2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1678/1678 [05:03<00:00,  5.52it/s, Epoch=1, LR=0.000665, Loss=0.445, TotalLoss=0.702]\n","100%|██████████| 602/602 [00:28<00:00, 21.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Step 1678, train_loss: 0.44506, val_loss: 1.22871, pearson: 0.80417\n","Model Inproved: 0.8011305231470817 ----> 0.8041684020635531\n","validation elasped time:  33.1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1678/1678 [05:03<00:00,  5.52it/s, Epoch=2, LR=0.000353, Loss=0.435, TotalLoss=0.591]\n","100%|██████████| 602/602 [00:28<00:00, 20.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Step 1678, train_loss: 0.43508, val_loss: 1.21518, pearson: 0.80492\n","Model Inproved: 0.8041684020635531 ----> 0.804920056408412\n","validation elasped time:  33.8\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1678/1678 [05:03<00:00,  5.53it/s, Epoch=3, LR=9.77e-5, Loss=0.143, TotalLoss=0.5]\n","100%|██████████| 602/602 [00:29<00:00, 20.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Step 1678, train_loss: 0.14349, val_loss: 1.21321, pearson: 0.79969\n","validation elasped time:  29.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1678/1678 [05:09<00:00,  5.41it/s, Epoch=4, LR=0, Loss=0.0451, TotalLoss=0.429]\n","100%|██████████| 602/602 [00:28<00:00, 20.78it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Step 1678, train_loss: 0.04515, val_loss: 1.21186, pearson: 0.79644\n","validation elasped time:  29.0\n","total elasped time:  1.7e+03\n","[0.8328216475590209, 0.8232072468655748, 0.8404750439073577, 0.804920056408412]\n","Mean: 0.8253559986850914\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# ----------------------------------------------\n","# Main Loop\n","# ----------------------------------------------\n","val_scores = []\n","oof_df = pd.DataFrame()\n","\n","for fold in range(CFG.num_fold): \n","    print(f\"*** FOLD {fold+1} / {CFG.num_fold}***\")\n","\n","    save_path = f\"{CFG.model_save_path}/model_{fold+1}.pth\"\n","\n","    train_data = train_df[train_df['fold'] != fold]\n","    valid_data = train_df[train_df['fold'] == fold]\n","    train_set = TrainDataset(train_data)\n","    valid_set = TrainDataset(valid_data)\n","\n","    train_loader = DataLoader(train_set,\n","                            batch_size=CFG.train_batch_size,\n","                            shuffle=True,\n","                            drop_last=True,\n","                            num_workers=2,\n","                            pin_memory=True)\n","    valid_loader = DataLoader(valid_set,\n","                            batch_size=CFG.valid_batch_size,\n","                            shuffle=False,\n","                            drop_last=False,\n","                            num_workers=2,\n","                            pin_memory=True)\n","\n","    model = PPPMModel().to(CFG.device)\n","    optimizer = create_optimizer_grouped_parameters(model)\n","    #optimizer = AdamW(model.parameters(), lr=CFG.learning_rate)\n","    scheduler = get_cosine_schedule_with_warmup(\n","        optimizer,\n","        num_training_steps=CFG.epochs*len(train_loader),\n","        num_warmup_steps=100\n","    )\n","\n","    val_score, val_preds = train_fn(model, save_path, train_loader, valid_loader, optimizer, scheduler=scheduler)\n","    val_scores.append(val_score)\n","    val_preds = label_to_score(val_preds)\n","    valid_data['preds'] = val_preds\n","    oof_df = pd.concat([oof_df, valid_data])\n","\n","    del model\n","    torch.cuda.empty_cache()\n","\n","    print(val_scores)\n","    print(\"Mean:\", np.array(val_scores).mean())\n","    if CFG.fold1_only == True:\n","        if fold == 0:\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nz0KfPOAXBbi","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1655336347693,"user_tz":-540,"elapsed":23,"user":{"displayName":"堂込一智","userId":"12219727497971505625"}},"outputId":"45d90be2-6d05-4d1e-a20d-0ea99d159fda"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'for batch_idx, data in enumerate(valid_loader):\\n  print(batch_idx)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}],"source":["\"\"\"for batch_idx, data in enumerate(valid_loader):\n","  print(batch_idx)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQUyBAm0NDiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655336347693,"user_tz":-540,"elapsed":8,"user":{"displayName":"堂込一智","userId":"12219727497971505625"}},"outputId":"54baadf4-f7d6-41da-899d-c241f75f92af"},"outputs":[{"output_type":"stream","name":"stdout","text":["fold0    0.832822\n","fold1    0.823207\n","fold2    0.840475\n","fold3    0.804920\n","oof      0.824622\n","dtype: float64\n"]}],"source":["scores = {f'fold{i}':j for i,j in enumerate(val_scores)}\n","scores['oof'] = np.corrcoef(oof_df['preds'], oof_df['score'])[0][1]\n","scores = pd.Series(scores)\n","print(scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmBZERwvNkCo"},"outputs":[],"source":["scores.to_csv(f'{CFG.out_path}/scores.csv')\n","oof_df.to_csv(f'{CFG.out_path}/oof_df.csv')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1nm9Mh83tpVkRtgzIW70V3KYDeQuT8pHY","timestamp":1655115771299},{"file_id":"1w5uFoGdqnkIBeoZRlOuJFwlnjjsNbRTK","timestamp":1654169874663},{"file_id":"1omHuxNQsXsp9AqAdUHsc_dt9KtP20yMP","timestamp":1654168824854},{"file_id":"1GP-aM4gbJLt_K4PjctRHRgU-p7AjxvC6","timestamp":1654167290824},{"file_id":"1PQsBMoKIQEQSVPuMOKwMFHmzcHbel78W","timestamp":1654165080313},{"file_id":"1DIjKU3ktcrs9RfESQSO4Xf2DNCAmSECb","timestamp":1653339588896},{"file_id":"1y0dkP43iXyooLt43fdk3ElIC6ndubdxd","timestamp":1653303566583},{"file_id":"1HgfHxBWVNXFQQ-VgAxmpLDpzfmcsO7U3","timestamp":1653263004769},{"file_id":"1Bi1AdwFa2IrEpkW_vMGZkOATD45BfjF5","timestamp":1653256357852},{"file_id":"1oY_J27W0Uv7QkVRwUZytT1mrvRgDXe4L","timestamp":1653203615891},{"file_id":"1cAftaWxxDtH2724hCJr8n0e_5Xhj8mPy","timestamp":1653125565109},{"file_id":"1ENNIPhZgUOS4XtGZ_7JWBH-731sVuHGM","timestamp":1653095567105},{"file_id":"1BlfrktS37Y0Gzb1HsRKJI3SrfqrlHIaL","timestamp":1653020571389},{"file_id":"1xoLboFCuSvLYtqoCVkPkypRFhm5-FQ93","timestamp":1653001405922}],"mount_file_id":"1Uet16e_JjpPbyK2arnGanjjlSSSDhiXH","authorship_tag":"ABX9TyOU6BCQIk2VRAy0CrDXCzyq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}